# ğŸ§­ Planner Agent Ã— Q-learning çµ±åˆãƒ¬ãƒãƒ¼ãƒˆ

## 1ï¸âƒ£ èƒŒæ™¯ã¨ç›®çš„

Planner Agentã¯ã€è¤‡æ•°ã®ã‚µãƒ–ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆï¼ˆDesigner, Coder, Deployerãªã©ï¼‰ã‚’çµ±æ‹¬ã—ã€
ã€Œã©ã®ã‚¿ã‚¹ã‚¯ã‚’ãƒ»ã©ã®é †ç•ªã§ãƒ»ã©ã®ãƒªã‚½ãƒ¼ã‚¹ã‚’ä½¿ã£ã¦å®Ÿè¡Œã™ã‚‹ã‹ã€ã‚’æ±ºå®šã™ã‚‹ä¸­æ ¸çš„å­˜åœ¨ã€‚
ã“ã‚Œã‚’é™çš„ãƒ«ãƒ¼ãƒ«ï¼ˆif-thenï¼‰ã§ã¯ãªã**å­¦ç¿’å‹æ„æ€æ±ºå®š**ã«å¤‰ãˆã‚‹ã“ã¨ã§ã€
ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆé€²è¡Œã®æœ€é©åŒ–ã‚’ç¶™ç¶šçš„ã«è¡Œã†ç‹™ã„ãŒã‚ã‚‹ã€‚

---

## 2ï¸âƒ£ Q-learningã®åŸºç¤

Q-learningã¯ã€**çŠ¶æ…‹(s)** ã¨ **è¡Œå‹•(a)** ã®çµ„ã¿åˆã‚ã›ã«å¯¾ã—ã¦ã€Œå°†æ¥ã®å ±é…¬æœŸå¾…å€¤Q(s,a)ã€ã‚’å­¦ç¿’ã™ã‚‹å¼·åŒ–å­¦ç¿’ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã€‚
ç’°å¢ƒãƒ¢ãƒ‡ãƒ«ã‚’å¿…è¦ã¨ã›ãšã€è¦³æ¸¬ã•ã‚ŒãŸå ±é…¬ã«åŸºã¥ã„ã¦æ–¹ç­–ã‚’æ”¹å–„ã—ã¦ã„ãã€‚

\[
Q(s,a) \leftarrow Q(s,a) + \alpha [r + \gamma \max_{a'} Q(s',a') - Q(s,a)]
\]

---

## 3ï¸âƒ£ Planner Agentã¸ã®é©ç”¨è¨­è¨ˆ

### ğŸ§© çŠ¶æ…‹ï¼ˆStateï¼‰

```python
state = {
  "task_progress": completion_ratio,
  "agent_load": avg_cpu_usage,
  "priority_level": project_priority,
  "deadline_gap": remaining_days,
  "reward_history": recent_reward_avg
}
```

### âš™ï¸ è¡Œå‹•ï¼ˆActionï¼‰

```python
actions = [
  "delegate_to_coder",
  "delegate_to_designer",
  "wait_for_feedback",
  "reprioritize_task",
  "request_external_api"
]
```

### ğŸ¯ å ±é…¬ï¼ˆRewardï¼‰

è©•ä¾¡åŸºæº–    å ±é…¬ã‚¹ã‚³ã‚¢ä¾‹
ã‚¿ã‚¹ã‚¯æˆåŠŸ    +1.0
ç´æœŸé…å»¶ãªã—    +0.5
ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ    -1.0
å†å®Ÿè¡Œå¿…è¦    -0.5
ä»–ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆæ”¯æ´æˆåŠŸ    +0.8

## 4ï¸âƒ£ å®Ÿè£…æ§‹é€ 

```text
planner_agent/
 â”œâ”€ q_learning/
 â”‚   â”œâ”€ q_table.py
 â”‚   â”œâ”€ policy.py
 â”‚   â””â”€ reward_model.py
 â”œâ”€ environment.py
 â”œâ”€ planner_core.py
 â””â”€ memory/q_memory.json
```

## 5ï¸âƒ£ å­¦ç¿’ãƒ«ãƒ¼ãƒ—

```python
for episode in range(max_episodes):
    state = env.reset()
    done = False
    while not done:
        action = policy.select_action(state)
        next_state, reward, done = env.step(action)
        q_table.update(state, action, reward, next_state)
        state = next_state
```

## 6ï¸âƒ£ åŠ¹æœã¨ãƒªã‚¹ã‚¯

### ğŸ’¡ åŠ¹æœ

ã‚¿ã‚¹ã‚¯å„ªå…ˆåº¦ã®è‡ªå‹•æœ€é©åŒ–

éå»ã®å¤±æ•—ãƒ‘ã‚¿ãƒ¼ãƒ³ã‹ã‚‰ã®è‡ªå·±ä¿®æ­£

ãƒãƒ«ãƒã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆé–“ã®å”èª¿å­¦ç¿’ï¼ˆFederated Q-learningåŒ–ï¼‰

### âš ï¸ ãƒªã‚¹ã‚¯

æ¢ç´¢è¡Œå‹•ã«ã‚ˆã‚‹åˆæœŸéåŠ¹ç‡

å ±é…¬ã‚¹ãƒ‘ãƒ¼ã‚¹ç’°å¢ƒã§ã®åœæ»

æ„æ€æ±ºå®šåŸºæº–ã®ãƒ–ãƒ©ãƒƒã‚¯ãƒœãƒƒã‚¯ã‚¹åŒ–

## 7ï¸âƒ£ å¼·åŒ–ãƒã‚¤ãƒ³ãƒˆ

è¦–ç‚¹    ææ¡ˆ
å ±é…¬æ§‹é€     GPTè©•ä¾¡ã‚„Notion DBã®æˆåŠŸãƒ­ã‚°ã‹ã‚‰è‡ªå‹•ã‚¹ã‚³ã‚¢åŒ–
çŸ¥è­˜å…±æœ‰    Planneré–“ã§Qãƒ†ãƒ¼ãƒ–ãƒ«ã‚’FederatedåŒæœŸ
å®‰å®šæ€§    Experience Replayã§å†å­¦ç¿’
é€æ˜æ€§    Qå€¤ã‚’Grafanaã§å¯è¦–åŒ–
ãƒ¡ã‚¿åˆ¶å¾¡    OrchestratorãŒå­¦ç¿’ç‡Î±ã‚’è‡ªå‹•èª¿æ•´

## 8ï¸âƒ£ LLM Ã— Q-learning ã®èåˆ

LLMã¯ã€Œã‚¿ã‚¹ã‚¯ã®æ„å‘³ç†è§£ã€ã«å¼·ãã€
Q-learningã¯ã€Œè¡Œå‹•ã®æœ€é©åŒ–ã€ã«å¼·ã„ã€‚
ä¸¡è€…ã‚’çµ„ã¿åˆã‚ã›ã‚‹ã“ã¨ã§ã€
â€œç†è§£ã—ã¦å‹•ãè‡ªå·±æœ€é©åŒ–ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆâ€ãŒå®Ÿç¾ã™ã‚‹ã€‚

## 9ï¸âƒ£ ç†è«–çš„æ ¹æ‹ 

Sutton & Barto, Reinforcement Learning: An Introduction (2018)

Silver et al., Mastering the Game of Go (Nature, 2016)

DeepMind, Hierarchical RL in Multi-Agent Systems (2021)

## ğŸ”Ÿ æ¬¡ã‚¢ã‚¯ã‚·ãƒ§ãƒ³

ã‚¹ãƒ†ãƒƒãƒ—    å†…å®¹    æˆæœç‰©
â‘     Q-tableæ§‹é€ ã®å®šç¾©    q_table.py
â‘¡    Rewardé–¢æ•°ä½œæˆ    reward_model.py
â‘¢    Îµ-greedyæ¢ç´¢ã®å®Ÿè£…    policy.py
â‘£    PlannerCoreã«hookçµ±åˆ    planner_core.py
â‘¤    å­¦ç¿’ãƒ­ã‚°ã‚’Notioné€£æº    planner_learning_log.md

## Reference

- docs/spec_os/srs.md
